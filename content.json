{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/04/06/hello-world/"},{"title":"【JSON】JSON的序列化与反序列化及常用的关于JSON的注解","text":"JSON的序列化与反序列化由于前后端分离项目的流行，前端需要JSON字符串，而后端需要JavaBean对象，这时候就需要前后端在交互的时候，能够将JSON字符串和JavaBean对象相互转换，由此引出了JSON的序列化与反序列化 简单来说： JSON的序列化指的就是将JavaBean对象转化为JSON格式的字符串 JSON的反序列化指的就是将JSON格式的字符串转化为JavaBean对象 什么是JavaBean对象？JavaBean本身就是一个类，它需要满足如下要求： 所有的类放在一个包中 类必须声明为public 类的属性必须要封装起来 需要提供get、set方法 至少存在一个无参构造方法 由此我们可以知道，一个POJO类，就是一个JavaBean对象，如： 123456789101112131415161718package com.hu.pojo; public class student{ private String name; private int age; public void setName(String name){ this.name = name; } public void setAge(int age){ this.age = age; } public String getName(){ return this.name; } public int getAge(){ return this.age; } } 常用的关于JSON的注解@JsonIgnoreProperties注解123// @JsonIgnoreProperties注解：是类注解，作用是json序列化时将java bean中的一些属性忽略掉，序列化和反序列化都受影响// ignoreUnknown = true : 忽略类中不存在的字段// value = {&quot;stackTrace&quot;, &quot;cause&quot;, &quot;message&quot;,&quot;localizedMessage&quot;, &quot;suppressed&quot;} @JsonCreator注解12// @JsonCreator注解，其作用是指定对象反序列化时的构造函数或者工厂方法，如果默认构造函数无法满足需求，// 或者说我们需要在构造对象时做一些特殊逻辑，可以使用该注解，需要搭配@JsonProperty使用 @JsonProperty注解1// @JsonProperty注解，其作用是把该属性的名称序列化为另外一个名称，如把trueName属性序列化为name，","link":"/2022/05/04/%E3%80%90JSON%E3%80%91JSON%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E5%8F%8A%E5%B8%B8%E7%94%A8%E7%9A%84%E5%85%B3%E4%BA%8EJSON%E7%9A%84%E6%B3%A8%E8%A7%A3/"},{"title":"【Java】ACM模式下的Java常用输入输出 I&#x2F;O（持续更新）","text":"样例一 输入1. BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));System.in 是向屏幕输入一些数据new InputStreamReader() 是输入流，可以用来读取数据new InputStreamReader(System.in) 以System.in作为参数，创建InputStreamReader的对象，就是读取屏幕上输入的数据BufferedReader类 使得读取具有缓冲功能 1BufferedReader reader = new BufferedReader(new InputStreamReader(System.in)); 2. int n = Integer.parseInt(reader.readLine());用于读入一个数字的情况 reader是BufferedReader类的对象，保存有读入的数据reader.readLine() 读取一行Integer.parseInt 将读取的这行数据（类型为String）转换为int类型 3. String[] strs = reader.readLine().split(“ “);reader.readLine() 读取一行reader.readLine().split(&quot; &quot;) 读取一行，将数据按空格分割开String[] strs = reader.readLine().split(&quot; &quot;); 读取一行，将数据按空格分割开并保存在String数组中 4. arr[i] = Integer.parseInt(strs[i]);Integer.parseInt() 由于读取的数据是String类型，按需要转为int类型 输出System.out.println() 12345678910111213141516171819202122232425import java.io.*;import java.util.*;public class testio { public static void main(String[] args) throws IOException { // 控制台读入数据并保存 BufferedReader reader = new BufferedReader(new InputStreamReader(System.in)); // 读入数据并赋值 int n = Integer.parseInt(reader.readLine()); // 读入数据并赋值 String[] str = reader.readLine().split(&quot; &quot;); // ------------------------------------------------------------------ // 使用数据 int[] a = new int[str.length]; for(int i = 0;i &lt; str.length;i++) a[i] = Integer.parseInt(str[i]); for(int num : a) System.out.print(num + &quot; &quot;); // for(int num : a) System.out.println(num); reader.close(); }}","link":"/2022/04/09/%E3%80%90Java%E3%80%91ACM%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84Java%E5%B8%B8%E7%94%A8%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA-I-O%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89/"},{"title":"【Kafka】Kafka基本架构","text":"=================生产者消费者相关================= 生产者Producer ：生产信息； 消费者Consumer ：订阅主题、消费信息； 代理Broker : 可以看作是一个独立的 Kafka 实例。多个 Kafka Broker 组成一个卡夫卡集群 Kafka Cluster； 主题topic：可以理解为一个队列， 生产者和消费者面向的都是一个 topic， Producer 将消息发送到特定的主题，Consumer 通过订阅特定的主题来消费消息； 分区partition： 为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。 =================备份相关================= 副本Replica： ：副本，为保证集群中的某个节点发生故障时，该节点上的 partition 数据不丢失，且 kafka 仍然能够继续工作，kafka 提供了副本机制，一个 topic 的每个分区都有若干个副本，一个 leader 和若干个 follower。 leader ：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 leader。 follower ：每个分区多个副本中的“从”，实时从 leader 中同步数据，保持和 leader 数据的同步。leader 发生故障时，某个 follower 会成为新的 follower。","link":"/2022/04/13/%E3%80%90Kafka%E3%80%91Kafka%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84/"},{"title":"【Kafka】Kafka如何保证消息不丢失、不重复？","text":"Kafka基本架构 生产者Producer ：生产信息； 消费者Consumer ：订阅主题、消费信息； 代理Broker : 可以看作是一个独立的 Kafka 实例。多个 Kafka Broker 组成一个卡夫卡集群 Kafka Cluster； 主题topic：可以理解为一个队列， 生产者和消费者面向的都是一个 topic， Producer 将消息发送到特定的主题，Consumer 通过订阅特定的主题来消费消息； 分区partition： 为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。 副本Replica： ：副本，为保证集群中的某个节点发生故障时，该节点上的 partition 数据不丢失，且 kafka 仍然能够继续工作，kafka 提供了副本机制，一个 topic 的每个分区都有若干个副本，一个 leader 和若干个 follower。 leader ：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 leader。 follower ：每个分区多个副本中的“从”，实时从 leader 中同步数据，保持和 leader 数据的同步。leader 发生故障时，某个 follower 会成为新的 follower。 Kafka如何保证消息不丢失、不重复Kafka消息的丢失和重复可能会发生在哪里？根据以上的Kafka架构图，我们推测一下，消息丢失可能会发生在哪里？ 生产者丢失数据 消费者丢失数据 那么消息重复消费可能会发生在哪里？ 同样也是在消费者端和生产者端，即： 生产者重复发送数据 消费者重复消费数据 如何保证消息的有序？ 同步发送模式：发出消息后，必须等待阻塞队列收到通知后，才发送下一条消息；同步发送模式可以保证消息不丢失、又能保证消息的有序性。 异步发送模式：生产者一直向缓冲区写消息，然后一起写到队列中；好处是吞吐量大，性能高。 Kafka如何保证生产者端的消息不丢失、不重复？生产者端丢失数据的情况分析 生产者端使用同步发送模式，有三种状态保证消息被安全生产，即配置acks参数（默认为1）： ● 1：集群的leader节点收到消息后，就可以发回成功写入的通知 ● 0：生产者在成功写入消息之前不会等待来着服务器的任何响应，即不在乎消息是否丢失了 ● 3：集群中的所有节点都收到消息时，才发回成功写入的通知，这种方式最可靠，但是性能最低 因此，当使用参数1且leader节点在写入参数时挂掉了，数据就会丢失。 使用异步模式时，当缓冲区满了，如果配置为0，则还没收到确认的情况下，缓冲区一满就会清空缓冲区中的消息，数据就丢失了 生产者端丢失数据的解决办法 在同步模式下，将发送消息的确认机制设置为-1，使得所有节点确认后再发送下一条数据即可 在异步模式下，如果消息发送出去了，但还没有收到确定的时候，在配置文件中设置成不限制阻塞超时的时间，即让生产者一直保持等待，也可以保证数据不丢失 生产者端重复发送数据的情况分析及解决办法重复发送数据，不用管，在消费者端建立去重表即可。 消费者端丢失数据的情况分析及解决办法如果消息在处理完成前就提交了offset，就有可能造成数据的丢失。解决办法是在后台提交位移前确保消息已经被正常处理了，然后手动提交offset（调用**commitSync()**函数）。 附注：offset（偏移量）是什么？Kafka中每一个分区partition都由一系列有序的、不可变的消息组成，这些消息被连续地追加到partition中，partition中的每个消息都有一个连续的序号，用于唯一标识一条消息，offset就记录着下一条将要发送给消费者的消息的序号，offset从语义上来看有两种： Current Offset Committed Offset Current Offset保存在消费者的客户端中，它表示希望收到的下一条消息的序号，Offset保证每次消费者消费信息时，都能收到正确位置的消息。 Committed Offset保存在broker上，它表示消费者已经确认消费过的消息的序号，这个确认需要消费者调用commitSync()函数，如果调用了这个函数，Commit Offset会被更新为Current Offset的值，如果没有调用这个函数，那么它就不会改变，依然是0。它保证新的消费者能够从正确的位置开始消费信息，从而避免重复消费。 消费者端重复消费数据的情况分析及解决办法在消费者端建立去重表即可保证消息只会被消费一次。 附注：Kafka的消息是消费完就消失的吗？kafka中的数据的删除和消费者是否消费无关，数据的删除只与kafka broker中 的数据保存时间（log.retention.hours=48，数据最大保存48小时）和数据最大保存内存（log.retention.bytes=1073741824，数据最大保存1G）有关。","link":"/2022/05/04/%E3%80%90Kafka%E3%80%91Kafka%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1%E3%80%81%E4%B8%8D%E9%87%8D%E5%A4%8D%EF%BC%9F/"},{"title":"【Linux】购买腾讯云服务器及环境配置，超详细","text":"1. 购买前须知腾讯学生云服务器_学生云主机_学生云数据库_云+校园特惠套餐 1.1 配置如何选择按需要选择自己的服务器，刚开始按1核2G的配置就可以了，如果内存只有1G可能会造成某些程序崩溃。配置也不需要太高，之后若网站等造成服务器流量访问开始上升，可以按需要升级配置。 注意： 在购买前关注一下腾讯云微信，如最近的虎年回馈活动，新人优惠力度很大，要使用好新人的优惠，之后再买会贵很多！ 1.2 轻量级云服务器和云服务器的差别在刚开始学习及起步阶段，我们更关心价格，等到服务器流量开始上升了，我们可以根据需要再升级服务器。（当服务器流量上升到需要升级服务器时，租赁服务器的费用也就不值一提了😜） 1.2.1 计费方式如果选择的是轻量级云服务器，采用的是包年包月 + 超额流量 + 自定义镜像 的计费方式，包年包月价格会比云服务器便宜一些，带宽会比云服务高一些，每个月有定量的流量包，类似于手机卡的流量包，流量使用在流量包内不需要额外的支付，但是如果超过流量包，需要支付超额流量费用，与手机超流量要支付超额费用一样。因此，购买轻量级云服务器的话，在流量包内进行使用，不需要再另外支付任何费用。 如果选择的是云服务器，那么会按照固定宽带或者按流量进行计费，按流量计费价格为1GB/0.8元，推荐将此费用修改为贷款拉满、按流量计费的方式，如果不能修改，需要联系客服进行修改，按流量计费上一种服务器降级策略，因此还会退还一部分费用。 1.2.2 详细对比详细对比见下表： 2. 初始化配置注： 博主购买的是轻量级应用服务器，系统为Ubuntu 20.04 以下配置仅供参考如果选择了Ubuntu20.04 + Docker20 那么在实例（你的服务器）创建成功后将自动按照配置创建Docker容器（相当于docker run）。也可以选择纯系统，然后在实例创建完成后再创建Docker容器。 新人优惠很划算，推荐有能力的在较大的优惠折扣下拉满使用年限，博主购买的配置和价格如下，购买期限为 5 年，每个月流量为300GB，仅供参考（提前退服务器会按使用时长退回相应的钱）： 3. 配置ssh免密登录注意！！：如果购买的是轻量级应用服务器的话，第一步需要修改密码（在购买的服务器的控制面板中），否则无法进行ssh免密登录 然后按以下步骤配置ssh免密登录： 进入购买的云服务器内,添加一个新用户 xxx ：sudo adduser xxx 输入密码 给该用户添加sudo权限：sudo usermod -aG sudo xxx 在AC-terminal (或者你的任一本地linux服务器)中配置服务器别名：vim .ssh/config，配置格式如下： 123Host 给你的云服务器命名,如myserver1 HostName 云服务的公网ip地址 User 上面创建的新用户xxx 本地创建密钥，如果以前创建过密钥，跳过此步：ssh-keygen 添加公钥到你的云服务器中：ssh-copy-id myserver1 输入云服务上的xxx用户密码即可 登录云服务器：ssh myserver1 可能发生的问题： 出现Permission denied (publickey) 原因：云服务器没有重置密码；解决办法：回到服务器控制面板重置密码 出现 WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! 原因：云端服务器的公钥发生改变；解决办法：将本地的authorized_keys这个文件下存放的远程主机公钥信息删除，再进行ssh-copy-id myserver1 4. 配置环境 更新ubuntu的安装源：sudo apt-get update 安装tmux：sudo apt-get install tmux 将tmux、vim等配置换成AC-terminal (或者你的任一本地linux服务器)的常用配置： 退回本地服务器：crtl + d 将配置文件发送到购买的服务器上：scp .bashrc .vimrc .tmux.conf myserver1: 登录回购买的服务器查看配置是否成功 5. 安装Docker如果之前选择了Ubuntu20.04 + Docker20 那么在实例（你的服务器）创建成功后将自动按照配置创建Docker容器，也就是说，不需要自己安装docker了，运行docker --version测试一下，如果出现了版本信息，则说明安装成功了。 否则按照官网方法一步步执行安装命令，注意，不要漏掉命令！一条条执行！ 官网！！安装需要一步步执行！！！！：官方网站_在 Ubuntu 上安装 Docker 引擎 （！！仅供参考！！）从此处开始（如果安装过docker，重新安装时需要卸载docker）注意：此处省略了中间的执行代码！！！！一定要去官网一步步执行！！！！注意：此处省略了中间的执行代码！！！！一定要去官网一步步执行！！！！注意：此处省略了中间的执行代码！！！！一定要去官网一步步执行！！！！注意：此处省略了中间的执行代码！！！！一定要去官网一步步执行！！！！ （！！仅供参考！！）执行到此处结束运行docker --version测试一下，如果出现了版本信息，则说明安装成功了。","link":"/2022/04/07/%E3%80%90Linux%E3%80%91%E8%B4%AD%E4%B9%B0%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8F%8A%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%EF%BC%8C%E8%B6%85%E8%AF%A6%E7%BB%86/"},{"title":"【Nginx】常见的负载均衡算法及限流算法","text":"常见的负载均衡算法1. 轮询算法按照时间顺序逐一轮换访问每台服务器。 2. 权重给服务器不同的权重，使得访问服务器的分布呈所给定的权重。 3. ip绑定法让来自同一ip地址的用户访问相同的服务器，可以有效解决动态网络存在的session共享问题。 Session和Cookies的区别 Session—存储在服务器端Session在服务器上的临时目录中创建一个文件，该文件用于存储已注册的Session变量。在访问期间，Session变量数据将可用于网站上的页面。当用户关闭浏览器或离开网站后，Session将立即终止，如果用户没有关闭浏览器或离开网站，那么服务器将也会在预定的时间段（通常为30分钟）后终止Session。 Cookie—存储在客户端Cookie是存储在客户端计算机上的文本文件,通常包含一些用户信息，例如名称，年龄或其他标识符号等。服务器将Cookie发送到浏览器，浏览器再将此信息存储在本地计算机上，以备将来使用。当下一次浏览器向Web服务器发送任何请求时，它会将这些Cookie信息发送到服务器，并且服务器使用该信息来识别用户，相当于可以跳过登录。 可以简单地将其看作key-value的关系，用户通过客户端上的Cookies访问到服务器端的Session 集群环境下的session共享问题某种服务部署在多台服务器上，假设用户第一次登陆，请求落在了1号服务器，那么1号服务器session中存了用户信息，那么下一次请求进来，落在了2号服务器，但是2号服务器并没有这个session，因此又会要求用户登陆一次，再存一次session信息。 4. 第三方插件fair通过第三方插件fair，可以实现按服务器的性能来动态地进行负载均衡，服务器响应短的会优先分配。 常见的限流算法限流是什么Nginx能够限制用户的访问请求速度，防止服务器资源消耗地过大 三种策略： 正常限制访问频率（正常限流）：限制用户的请求，即Nginx多久能接受到一个请求 突发限制访问频率（突发流量）： Nginx提供burst参数结合nodelay参数可以解决流量突发的问题，可以设置能处理的超过设置的请求数外能额外处理的请求数。先处理该请求的前几个请求，多的如果没有别人请求就执行，否则漏掉 限制并发连接数：如单个IP同时并发连接数最多只能10个连接，整个虚拟服务器同时最大并发数最多只能100个链接。只有当请求的被服务器处理后，虚拟服务器的连接数才会计数。 漏桶算法 令牌桶算法","link":"/2022/04/12/%E3%80%90Nginx%E3%80%91%E5%B8%B8%E8%A7%81%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%AE%97%E6%B3%95%E5%8F%8A%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{"title":"【Redis】 主从复制及原理探析","text":"定义读写分离 主从复制！ 主机master处理写操作、从机follower处理读操作，因为80%的操作都是读操作，我们就将读和写进行分离，从而减缓服务器的压力！一般是一主二从，构成一个Redis集群。因此主机的数据要复制到其他的从机中，这就引出了主从复制的概念： 作用主从复制的作用包括： 数据冗余：实现了数据的热备份 负载均衡：用多台服务器承担高负担的读操作，分担服务器负载 故障恢复：当主机出现问题时，可以由从节点提供服务，快速地恢复可用（哨兵模式） 高可用的基石 主从复制的工作流程（三阶段）准备阶段（建立连接）设置主机与从机，默认每台Redis服务器都是主机，常用的配置方法有两种： 命令行配置 redis-server redis_config/redis-6380.conf --slaveof 127.0.0.1 6379 ：这样做会导致从机如果意外宕机，再连线后就不是原来主机的从机了，而是会恢复成单独的主机 config文件配置 vim redis-6380.conf 然后添加slaveof 127.0.0.1 6379：是一种永久化的配置，即若从机宕机，再连接也还是配置主机的从机 数据同步阶段（全量复制）从机在刚连接到主机的时候会进行一次全量复制，即会将主机的所有数据复制给从机。全量复制的过程如下： 创建命令缓冲区 生成rdb文件（即持久化文件），通过socket发送给从机 从机接收rdb文件，执行rdb恢复过程 命令传播阶段（增量复制）当master数据库状态被修改后，导致主从服务器数据库状态不一致，此时需要让主从数据同步到一致的状态，同步的动作称为命令传播，此时的复制称为增量复制，即将后序更新的命令传给从机，完成同步操作。 哨兵模式哨兵模式解决的是“如果主机宕机了怎么办？”的问题。 哨兵实际上就是一个进程，它通过发送信息给各个服务器来判断各服务器的状态，然而一个哨兵可能会发生问题，于是可以用其他的哨兵互相监督，这样就形成了多哨兵模式，如：哨兵不仅监督服务器是否宕机，还负责监考其他哨兵是否还存活。 如果一个哨兵监测到了主机宕机，并不会马上进行投票操作（主观下线），当后面的哨兵监测到主服务宕机达到一定的数量后，那么哨兵之间会进行一次投票（投票算法），投票的结果由某一个哨兵发起，进行故障转移操作，切换主机成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从机切换为主机（客观下线）。主机连接回来后也没有任何从机了，在哨兵模式下，主机回来后，会自动变为新主机的从机！（😥）","link":"/2022/04/11/%E3%80%90Redis%E3%80%91%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%8F%8A%E5%8E%9F%E7%90%86%E6%8E%A2%E6%9E%90/"},{"title":"【Redis】缓存穿透、缓存击穿、缓存雪崩及解决办法剖析","text":"缓存穿透定义：什么是缓存穿透当我们查询一个数据的时候，我们会首先查询Redis数据库中是否有该数据，如果没有的话才会向持久化数据库中进行查找（一般是MySQL数据库），如果我们的同时访问该数据的用户数量过多（如双十一秒杀活动，千万条请求会涌向同一个数据），且在某个时间段内突然都向我们的应用服务器发送请求，此时我们的应用服务器压力就会变大，然后他会不停地去查Redis的缓存数据，然后发现没有这条数据，于是向MySQL数据库发起请求，过大的访问请求就会导致数据库崩溃，这个过程我们就把他叫做缓存穿透。 缓存穿透是存在于Redis数据库中没有这条数据，而又同时过大的访问请求访问这条数据导致的！ 解决办法布隆过滤器 布隆过滤器的原理布隆过滤器（Bloom Filter）是1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。其原理是哈希映射。 优点： 空间效率和查询时间都远远超过一般的算法。缺点： 有一定的误识别率（1%左右，这是由于存在哈希冲突），删除困难。 布隆过滤器如何使用对数据的id进行三次哈希操作，在对应映射的位置修改值：如果在 Hash 后，原始位它是 0 的话，将其从 0 变为 1；如果本身这一位就是 1 的话，则保持不变。 布隆过滤器如何解决缓存穿透现象？当查询一件商品的缓存信息时，我们首先要判断这件商品是否存在： 通过三个哈希函数对数据id计算哈希值 然后，在布隆数组中查找访问对应的位值，0或1 判断，三个值中，只要有一个不是1，那么我们认为数据是不存在的 如果不存在，则不会向数据库中进行访问操作，这样就大大减少了数据库的访问压力 注意： 布隆过滤器只能精确判断数据不存在情况，对于存在我们只能说是可能，因为存在Hash冲突情况 缓存空对象 缓存击穿缓存击穿是指一个key非常热门（微博热搜），在不断地扛着极高的并发，大并发集中对这一个点进行访问，当Key在Redis中突然失效的一瞬间，高并发会直接砸到（⚡）MySQL服务器上，导致数据库瞬间压力过大。 缓存击穿强调的是 Key 突然失效导致的服务器崩溃现象。 解决办法设置热点数据永不过期直接在Redis服务器中设置该数据永远不会过期即可。 给MySQL访问加互斥锁给MySQL服务器中这条数据加上互斥锁，在Redis服务器还未恢复这条数据时，无论有多大的流量，都只有一个人能访问数据，降低了服务器处理高并发的风险。 缓存雪崩定义：什么是缓存雪崩缓存雪崩是指的是在某一个时间段Redis服务器缓存大批量失效，而查询数据量巨大，引起Redis数据库压力过大宕机，然后大批量请求涌入MySQL服务器导致其宕机的现象。 解决办法Redis高可用（设计Redis集群，即主从复制）设计Redis集群，当一台挂掉后还有其他的服务器能抗住压力（💪） 限流降级在缓存失效后，通过加锁（🔐）或者队列来控制读数据库的线程的数量，比如对某一个key同一时间只能有一条线程访问和写数据，其他线程进行等待。 数据预热在正式部署前，将可能的热点key加载到缓存中，并设置不同的失效时间，让key值失效的时间尽可能均匀。","link":"/2022/04/11/%E3%80%90Redis%E3%80%91%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E3%80%81%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E3%80%81%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%E5%89%96%E6%9E%90/"},{"title":"【SpringBoot】自动装配原理分析","text":"什么是自动装配我们在使用SpringBoot的时候，SpringBoot会自动帮我们进行如下配置： 自动配置好Tomcat 自动配置好SpringMVC 自动配置好Web常见功能，如字符编码问题 默认的包结构 各种配置拥有默认值 按需加载所有自动配置项等等 自动装配原理分析@SpringBootApplication首先找到 SpringBoot 的主启动类，在这个主启动类上存在有一个 @SpringBootApplication 的注解，SpringBoot 通过这个注解来说明这个类是主启动类，然后通过执行类中的main方法来启动 SpringBoot 项目 在main方法里面会执行一个run方法，它会使用一个默认的配置来启动SpringApplication程序 然后看一下注解@SpringBootApplication究竟做了什么，它主要有如下两个重要注解： @SpringBootConfiguration @EnableAutoConfiguration @SpringBootConfiguration在@SpringBootConfiguration中，有这样一个注解**@Configuration**点开 @Configuration 注解，我们发现，它实际上就是一个 @Component 注解，代表它是 SpringBoot 的一个组件，代表它是一个配置类。因此， @Configuration 注解仅代表其是一个配置类，类似的有： @EnableAutoConfiguration点开 @EnableAutoConfiguration 注解，我们发现除了元注解外，它主要有两个注解： @AutoConfigurationPackage @Import(AutoConfigurationImportSelector.class) @AutoConfigurationPackage先来看 @AutoConfigurationPackage 注解，点开它，我们发现主要的注解是@Import(AutoConfigurationPackages.Registrar.class) 事实上，@Import就是给容器中导入组件，@Import(AutoConfigurationPackages.Registrar.class)就是给容器中导入Registrar组件 @Import(AutoConfigurationPackages.Registrar.class)通过import注解导入了一个AutoConfigurationPackages下的一个静态内部类Registrar，它的作用是给容器中批量注册组件` 在Registrar类中，重写了方法registerBeanDefinitions，大概就是说注册定义的Bean，他要扫描的包是我们的主启动类，这时候就大概知道SpringBoot 就是通过这个注解来扫描主启动类下的包和子包下的所有组件并加载到 Spring 容器。 @Import(AutoConfigurationImportSelector.class)点进这个注解，我们发现他重写了selectImports方法 而在selectImports方法中，它又调用了getAutoConfigurationEntry方法 当走完getCandidateConfigurations时，它会加载很多configurations，这些都是自动配置类的全限定名。 让我们来梳理一下现在已经知道的信息：SpringBoot 通过 @Import(AutoConfigurationImportSelector.class) 这个注解加载了我们 Spring 容器需要的所有配置类，可以知道这些配置类是通过getCandidateConfigurations()方法加载的，继续点进getCandidateConfigurations()方法 下面通过一个SpringFactoriesLoader调用了一个loadFactoryNames方法，点进SpringFactoriesLoader我们发现有一个工厂资源地址 因此在getCandidateConfigurations()方法中，它通过SpringFactoriesLoader.loadFactoryNames方法扫描到META-INF/spring.factories的jar包，并且将需要的自动配置包加载到 Spring 容器中 找到这个工厂资源地址： 发现这里面全部是全限定名，所以就是在这里找到了自动配置类并且加载到了 Spring 容器中，完成自动配置类的加载。","link":"/2022/04/12/%E3%80%90SpringBoot%E3%80%91%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/"},{"title":"【SpringMVC】 DispatcherServlet工作流程","text":"SpringMVC的核心是DispatcherServlet，DispatcherServlet，即分发器、前端控制器，其功能是针对用户给定的url请求进行接收、拦截、分发和调度。根据上图，做一个简要的介绍： 用户发来一个url请求，如http://localhost:8080/SpringMVC/hello DispatcherServlet接收并拦截下url地址，并将其拆分为三部分： 服务器域名：http://localhost:8080/SpringMVC/hello 部署在服务器上的web站点：http://localhost:8080/`SpringMVC`/hello 控制器Controller：http://localhost:8080/SpringMVC/`hello` 然后DispatcherServlet调用HandlerMapping处理器映射，通过url信息查找Handler HandlerExecution表示查找到的具体的Handler，其作用是根据url信息查找控制器Controller，如上式url中的hello HandlerExecution将查找到的Controller信息返回给DispatcherServlet HandlerAdapter表示处理器适配器，其按照特定的规则去执行Handler Handler让具体的Controller执行（业务层代码） Controller将执行后的信息，如ModelAndView对象返回给HandlerAdapter HandlerAdapter将视图逻辑名或模型传递给DispatcherServlet DispatcherServlet调用ViewResolver视图解析器来解析逻辑视图名 DispatcherServlet根据解析好的逻辑视图名来调用具体的视图 View将信息渲染到页面上 传递给用户","link":"/2022/04/11/%E3%80%90SpringMVC%E3%80%91DispatcherServlet%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/"},{"title":"【Spring】IoC的理解及三种依赖注入方式","text":"IoC是什么意思IoC，即控制反转，依赖注入（DI）是SpringIoC的一个具体体现，因此，我们可以通过DI（依赖注入）来理解什么是IoC。 要理解什么是依赖注入，首先就要知道什么是依赖。 依赖定义： 例如在类A中使用了类B的实例化对象，即我们在类A中，用new关键字显示地定义了一个类B的对象，那么就说类A依赖类B。 这样有什么不好？这种依赖的方法，会造成严重的耦合性，如果类B发生了变化，类A也需要发生相应的变换，这样是非常影响应用的开发和相关维护的。那么，有没有一种方式能够使得类与类之间的依赖关系不这么紧密呢？ 控制反转在计算机中，有这样一句话，没有什么是加一层解决不了的，因此，我们引入了Spring的IoC容器，我们将类的实例化交给SpingIoC容器来执行，这样一来，实例化对象的权利就由用户转交给了Spring容器，这就叫作控制反转！ Spring提供的依赖注入的三种方式setter注入（属性注入） 构造一个pojo实体类 在xml文件中绑定这个pojo实体类，并用property完成属性注入 在需要依赖注入的地方调用ApplicationContext类的对象来调用Bean容器中的依赖对象（创建对象的过程由程序员本身转移到了框架中，此为控制反转），完成属性注入 构造器注入 构建pojo实体类和有参构造方法 配置xml文件，绑定实体类，使用constructor-arg完成有参构造方法注入 调用ApplicationContext的实例化对象调用Bean容器中用有参构造器创建的对象（创建对象的过程由程序员本身转移到了框架中，此为控制反转）完成注入 p命名空间注入（工厂方法注入） 定义pojo实体类，完成setter方法 完成xml文件配置，注意导入p命名空间的头文件：xmlns:p=&quot;http://www.springframework.org/schema/p&quot; 使用ApplicationContext的实例化对象调用Bean容器中的对象完成依赖注入 @Autowired@Autowired是一种注解，可以对成员变量、方法和构造函数进行标注，来完成自动装配的工作；@Autowired标注可以放在成员变量上，也可以放在成员变量的set方法上，也可以放在任意方法上表示，自动执行当前方法，如果方法有参数，会在IOC容器中自动寻找同类型参数为其传值。","link":"/2022/04/11/%E3%80%90Spring%E3%80%91IoC%E7%9A%84%E7%90%86%E8%A7%A3%E5%8F%8A%E4%B8%89%E7%A7%8D%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5%E6%96%B9%E5%BC%8F/"},{"title":"【算法】根据不同的数据范围反推算法","text":"在ACM模式下（笔试一般也是ACM模式），代码的操作数控制在10^8^左右可以保证不会超时，因此，基于这个理论，我们可以通过给出的数据的范围加上各种算法的时间复杂度，对这题可能会使用的算法进行一个简单的推测，具体如下所示：","link":"/2022/04/17/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E6%A0%B9%E6%8D%AE%E4%B8%8D%E5%90%8C%E7%9A%84%E6%95%B0%E6%8D%AE%E8%8C%83%E5%9B%B4%E5%8F%8D%E6%8E%A8%E7%AE%97%E6%B3%95/"},{"title":"公告","text":"非常抱歉！目前博客还尚未完成全部迁移，请移步左下方CSDN链接查看更多文章😊。","link":"/2022/04/07/%E5%85%AC%E5%91%8A/"}],"tags":[{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"json","slug":"json","link":"/tags/json/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"kafka","slug":"kafka","link":"/tags/kafka/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"springboot","slug":"springboot","link":"/tags/springboot/"},{"name":"springmvc","slug":"springmvc","link":"/tags/springmvc/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"公告","slug":"公告","link":"/tags/%E5%85%AC%E5%91%8A/"}],"categories":[{"name":"Hexo","slug":"Hexo","link":"/categories/Hexo/"},{"name":"技术原理及源码分析","slug":"技术原理及源码分析","link":"/categories/%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"Java基础","slug":"Java基础","link":"/categories/Java%E5%9F%BA%E7%A1%80/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"算法与数据结构","slug":"算法与数据结构","link":"/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]}